{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Using the Expectation-Maximization Algorithm\n",
    "\n",
    "In this notebook we demonstrate how to train the mixture lymphatic progression models. We do this for a simple set of synthetic data and see if and how well we can recover the original parameters that we set.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from typing import Literal, Any, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lymixture import LymphMixture\n",
    "from lymixture import utils\n",
    "from lymph.models import Unilateral\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data\n",
    "\n",
    "Define parameters and configuration to draw a number of synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modality = namedtuple(\"Modality\", [\"spec\", \"sens\"])\n",
    "\n",
    "# definition of the directed acyclic graph\n",
    "GRAPH_DICT = {\n",
    "    (\"tumor\", \"T\"): [\"II\", \"III\"],\n",
    "    (\"lnl\", \"II\"): [\"III\"],\n",
    "    (\"lnl\", \"III\"): [],\n",
    "}\n",
    "# definition of the diagnostic modality\n",
    "MODALITIES = {\n",
    "    \"path\": Modality(spec=0.9, sens=0.9),\n",
    "}\n",
    "# assumed distributions over the time to diagnosis\n",
    "DISTRIBUTIONS = {\n",
    "    \"early\": utils.binom_pmf(k=np.arange(11), n=10, p=0.3),\n",
    "    \"late\": utils.late_binomial,\n",
    "}\n",
    "\n",
    "# params of component 1\n",
    "PARAMS_C1 = {\n",
    "    \"TtoII_spread\": 0.05,\n",
    "    \"TtoIII_spread\": 0.25,\n",
    "    \"IItoIII_spread\": 0.5,\n",
    "    \"late_p\": 0.5,\n",
    "}\n",
    "# params of component 2\n",
    "PARAMS_C2 = {\n",
    "    \"TtoII_spread\": 0.25,\n",
    "    \"TtoIII_spread\": 0.05,\n",
    "    \"IItoIII_spread\": 0.1,\n",
    "    \"late_p\": 0.5,\n",
    "}\n",
    "SUBSITE_COL = (\"tumor\", \"1\", \"subsite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModalityDict = dict[str, dict[str, float | Literal[\"clinical\", \"pathological\"]]]\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    model_kwargs: dict[str, Any] | None = None,\n",
    "    modalities: ModalityDict | None = None,\n",
    "    distributions: dict[str, list[float] | Callable] | None = None,\n",
    ") -> Unilateral:\n",
    "    \"\"\"Create a model to draw patients from.\"\"\"\n",
    "    model = Unilateral(**(model_kwargs or {\"graph_dict\": GRAPH_DICT}))\n",
    "\n",
    "    for name, modality in (modalities or MODALITIES).items():\n",
    "        model.set_modality(name, modality.spec, modality.sens)\n",
    "\n",
    "    for t_stage, dist in (distributions or DISTRIBUTIONS).items():\n",
    "        model.set_distribution(t_stage, dist)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_datasets(\n",
    "    model: Unilateral,\n",
    "    num_c1: int,\n",
    "    num_c2: int,\n",
    "    num_c3: int,\n",
    "    tstage_ratio: float,\n",
    "    mix: float,\n",
    "    rng: np.random.Generator,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Draw patients for the three datasets.\"\"\"\n",
    "    model.set_params(**PARAMS_C1)\n",
    "    c1_data = model.draw_patients(\n",
    "        num=num_c1 + int(num_c3 * mix),\n",
    "        stage_dist=[tstage_ratio, 1 - tstage_ratio],\n",
    "        rng=rng,\n",
    "    )\n",
    "    model.set_params(**PARAMS_C2)\n",
    "    c2_data = model.draw_patients(\n",
    "        num=num_c2 + int(num_c3 * (1 - mix)),\n",
    "        stage_dist=[tstage_ratio, 1 - tstage_ratio],\n",
    "        rng=rng,\n",
    "    )\n",
    "    c3_data = pd.concat(\n",
    "        [\n",
    "            c1_data.iloc[num_c1:],\n",
    "            c2_data.iloc[num_c2:],\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "        axis=0,\n",
    "    )\n",
    "    c1_data = c1_data.iloc[:num_c1]\n",
    "    c2_data = c2_data.iloc[:num_c2]\n",
    "\n",
    "    c1_data[SUBSITE_COL] = \"c1\"\n",
    "    c2_data[SUBSITE_COL] = \"c2\"\n",
    "    c3_data[SUBSITE_COL] = \"c3\"\n",
    "\n",
    "    return pd.concat([c1_data, c2_data, c3_data], ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "synthetic_data = draw_datasets(\n",
    "    model=model,\n",
    "    num_c1=1000,\n",
    "    num_c2=1000,\n",
    "    num_c3=1000,\n",
    "    tstage_ratio=0.4,\n",
    "    mix=0.5,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "random_idx = rng.choice(synthetic_data.index, size=6, replace=False)\n",
    "synthetic_data.iloc[random_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Now, we define the mixture model and load the just drawn data. Note that we use only two components, hoping that the `\"c3\"` subgroup can be described as a mixture of these two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = {\n",
    "    (\"tumor\", \"T\"): [\"II\", \"III\"],\n",
    "    (\"lnl\", \"II\"): [\"III\"],\n",
    "    (\"lnl\", \"III\"): [],\n",
    "}\n",
    "num_components = 2\n",
    "\n",
    "mixture = LymphMixture(\n",
    "    model_cls=Unilateral,\n",
    "    model_kwargs={\"graph_dict\": graph},\n",
    "    num_components=num_components,\n",
    "    universal_p=False,\n",
    ")\n",
    "mixture.load_patient_data(\n",
    "    synthetic_data,\n",
    "    split_by=(\"tumor\", \"1\", \"subsite\"),\n",
    "    mapping=lambda x: x,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the diagnostic modality to be the same as in the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, modality in MODALITIES.items():\n",
    "    mixture.set_modality(name=name, spec=modality.spec, sens=modality.sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the distribution over diagnosis times. Again, we set this to be the same as during the synthetic data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_stage, dist in DISTRIBUTIONS.items():\n",
    "    mixture.set_distribution(t_stage, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lymixture.em import expectation, maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterative steps of computing the expectation over the latent variables (E-step) and maximizing the model parameters (M-step) can be initialized with an arbitrary set of starting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {k: rng.uniform() for k in mixture.get_params()}\n",
    "mixture.set_params(**params)\n",
    "mixture.normalize_mixture_coefs()\n",
    "log_resps = utils.normalize(rng.uniform(size=mixture.get_resps().shape).T, axis=0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a function to check the convergence of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_converged(\n",
    "    history: list[dict[str, float]],\n",
    "    rtol: float = 1e-4,\n",
    ") -> bool:\n",
    "    \"\"\"Check if the EM algorithm has converged.\"\"\"\n",
    "    if len(history) < 2:\n",
    "        return False\n",
    "\n",
    "    old, new = history[-2][\"llh\"], history[-1][\"llh\"]\n",
    "    return np.isclose(old, new, rtol=rtol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can iterate the computation of the expectation value of the latent variables (E-step) and the maximization of the (complete) data log-likelihood w.r.t. the model parameters (M-step).\n",
    "\n",
    "While the algorithm converges, we check the incomplete data likelihood after each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "snapshot = {\n",
    "    \"llh\": mixture.incomplete_data_likelihood(),\n",
    "    **mixture.get_params(as_dict=True, as_flat=True),\n",
    "}\n",
    "history = [snapshot]\n",
    "\n",
    "while not is_converged(history, rtol=1e-4):\n",
    "    print(f\"iteration {count:>3d}: {history[-1]['llh']:.3f}\")\n",
    "    count += 1\n",
    "\n",
    "    log_resps = expectation(mixture, params, log=True)\n",
    "    mixture.set_resps(np.exp(log_resps))\n",
    "    assert np.allclose(np.exp(np.logaddexp.reduce(log_resps, axis=1)), 1.0)\n",
    "    params = maximization(mixture, log_resps)\n",
    "\n",
    "    snapshot = {\n",
    "        \"llh\": mixture.incomplete_data_likelihood(),\n",
    "        **mixture.get_params(as_dict=True, as_flat=True),\n",
    "    }\n",
    "    history.append(snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "After convergence, we can have a look at the likelihood and the parameters during the iterations. Ideally, the likelihood increases strictly monotonically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history)\n",
    "history_df.plot(\n",
    "    y=[\"llh\", \"0_TtoII_spread\", \"1_TtoII_spread\"],\n",
    "    subplots=[(\"llh\",), (\"0_TtoII_spread\", \"1_TtoII_spread\")],\n",
    "    sharex=True,\n",
    "    xlim=(0, None),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, more importantly, let's also see if the learned parameters reproduce what we put into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = {}\n",
    "fixed_params.update({f\"0_{name}\": value for name, value in PARAMS_C1.items()})\n",
    "fixed_params.update({f\"1_{name}\": value for name, value in PARAMS_C2.items()})\n",
    "\n",
    "learned_params = mixture.get_params(as_dict=True, as_flat=True)\n",
    "\n",
    "for name, fixed in fixed_params.items():\n",
    "    learned = learned_params[name]\n",
    "    print(f\"{name:>16s}: {fixed = :.3f}, {learned = :.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
